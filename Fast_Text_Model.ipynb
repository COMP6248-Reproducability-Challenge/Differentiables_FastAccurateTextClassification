{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "fast_text_model_dl_ac.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.1"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "RhlVjbx-Zh9Q",
        "colab": {}
      },
      "source": [
        "# Execute this code block to install dependencies when running on colab\n",
        "try:\n",
        "    import torch\n",
        "except:\n",
        "    from os.path import exists\n",
        "    from wheel.pep425tags import get_abbr_impl, get_impl_ver, get_abi_tag\n",
        "    platform = '{}{}-{}'.format(get_abbr_impl(), get_impl_ver(), get_abi_tag())\n",
        "    cuda_output = !ldconfig -p|grep cudart.so|sed -e 's/.*\\.\\([0-9]*\\)\\.\\([0-9]*\\)$/cu\\1\\2/'\n",
        "    accelerator = cuda_output[0] if exists('/dev/nvidia0') else 'cpu'\n",
        "\n",
        "    !pip install -q http://download.pytorch.org/whl/{accelerator}/torch-1.0.0-{platform}-linux_x86_64.whl torchvision\n",
        "    ! pip install --upgrade git+https://github.com/sovrasov/flops-counter.pytorch.git\n",
        "\n",
        "try:\n",
        "    import torchtext\n",
        "except:\n",
        "    !pip install torchtext\n",
        "    \n",
        "try:\n",
        "    import spacy\n",
        "except:\n",
        "    !pip install spacy\n",
        "    \n",
        "try:\n",
        "    spacy.load('en')\n",
        "except:\n",
        "    !python -m spacy download en"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPLMTSzD3p1f",
        "colab_type": "text"
      },
      "source": [
        "# Data loading and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "j-T4VsjoZlYt",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "from torchtext import data\n",
        "import torch.nn.functional as F\n",
        "from torch import nn\n",
        "from torch import optim\n",
        "from torch.distributions import Categorical\n",
        "from torch.distributions import Binomial\n",
        "from torchtext import datasets\n",
        "import os.path\n",
        "\n",
        "import random\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "i9C3tPlZZrBG",
        "outputId": "2af3c5bf-66de-4672-c77f-f37ad97095ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 50
        }
      },
      "source": [
        "TEXT = data.Field(tokenize='spacy', lower=True, include_lengths=True)\n",
        "LABEL = data.LabelField(dtype=torch.float)\n",
        "_train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz:   0%|          | 164k/84.1M [00:00<00:57, 1.47MB/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:14<00:00, 5.93MB/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "anWyjQAwZrvi",
        "outputId": "f8141b26-c40e-4a63-f622-cd4fe3bb7d7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "train_data, valid_data = _train_data.split(0.8)\n",
        "\n",
        "print(f'Number of training examples: {len(train_data)}')\n",
        "print(f'Number of validation examples: {len(valid_data)}')\n",
        "print(f'Number of testing examples: {len(test_data)}')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of training examples: 20000\n",
            "Number of validation examples: 5000\n",
            "Number of testing examples: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tjjynYwDcgBj",
        "outputId": "39d2a679-e3e6-4288-85da-0e993f448c40",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 67
        }
      },
      "source": [
        "TEXT.build_vocab(train_data, max_size=100000, vectors=\"glove.6B.100d\")\n",
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
        "print(f\"Unique tokens in LABEL vocabulary: {len(LABEL.vocab)}\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [03:29, 4.12MB/s]                           \n",
            "100%|█████████▉| 398204/400000 [00:19<00:00, 20314.70it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Unique tokens in TEXT vocabulary: 91156\n",
            "Unique tokens in LABEL vocabulary: 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SoSIIOpmlhsR",
        "outputId": "aa348c17-1d92-4d3c-ef4c-eed4d4975628",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "device = 'cpu'\n",
        "# Assume that we are on a CUDA machine, then this should print a CUDA device:\n",
        "\n",
        "print(device)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TJ-krEODTunN"
      },
      "source": [
        "# Model and training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wQnA9EazmYT9",
        "colab": {}
      },
      "source": [
        "BATCH_SIZE = 50\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size=BATCH_SIZE,\n",
        "    device=device,\n",
        "    shuffle = False,\n",
        "    sort_key=lambda x: len(x.text),\n",
        "    sort_within_batch=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "iis6f53adVHR",
        "colab": {}
      },
      "source": [
        "eps = torch.tensor(1e-9)\n",
        "temp = []\n",
        "R = 20 # chunk of words read or skipped\n",
        "\n",
        "def reward_function(prob, true_label):\n",
        "    \"\"\"\n",
        "    Returns 1 if correct prediction, -1 otherwise\n",
        "    \"\"\"\n",
        "    # print(\"true_label\", \"prob\", true_label, prob)\n",
        "    if prob>0.5 and true_label>0.5:\n",
        "        return torch.tensor(1.0, requires_grad=True)\n",
        "    if prob<0.5 and true_label<0.5:\n",
        "        return torch.tensor(1.0, requires_grad=True)\n",
        "    return torch.tensor(-1.0, requires_grad=True)\n",
        "\n",
        "def sample_binary(prob):\n",
        "    if prob>random.random:\n",
        "        return torch.tensor(1)\n",
        "    return torch.tensor(0)\n",
        "\n",
        "class SkipReadingModel(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, gamma=0.99, train_mode=True, K=4):\n",
        "        super().__init__()\n",
        "        \n",
        "        # store dimensions and constants\n",
        "        self.input_dim = input_dim\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.output_dim = output_dim\n",
        "        self.gamma = torch.tensor(gamma)\n",
        "        self.train_mode = train_mode\n",
        "        self.K = K\n",
        "        \n",
        "        # create layers\n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        self.lstm_cell = nn.LSTMCell(input_size = embedding_dim, hidden_size = hidden_dim, bias = True)\n",
        "        self.stop_linear_1 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.stop_linear_2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.stop_linear_3 = nn.Linear(hidden_dim, 1)\n",
        "        \n",
        "        self.jumping_linear_1 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.jumping_linear_2 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.jumping_linear_3 = nn.Linear(hidden_dim, K)\n",
        "        \n",
        "        self.output_linear_1 = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.output_linear_2 = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "        self.value_head = nn.Linear(hidden_dim, 1)\n",
        "        \n",
        "        # Baseline weight        \n",
        "        self.wb = nn.Parameter(data=torch.zeros(self.hidden_dim), requires_grad=True)\n",
        "        self.cb = nn.Parameter(data=torch.tensor((0.0)), requires_grad=True)\n",
        "        \n",
        "        # Initialize lstm_cell states\n",
        "        self.initialize_lstm_cell_states()\n",
        "        \n",
        "        # Initalize episode number and time number\n",
        "        self.initialize_for_new_batch()\n",
        "        self.initialize_time_number()\n",
        "        \n",
        "        # Overall reward and loss history\n",
        "        self.reward_history = []\n",
        "        self.loss_history = []\n",
        "        self.training_accuracies = []\n",
        "        self.validation_accuracies = []\n",
        "        # torch.tensor((0.0), requires_grad=True) \n",
        "    \n",
        "    def initialize_lstm_cell_states(self):\n",
        "        self.c = torch.zeros(1, self.hidden_dim, requires_grad=True)\n",
        "        self.h = torch.zeros(1, self.hidden_dim, requires_grad=True)\n",
        "            \n",
        "    def initialize_episode_number(self):\n",
        "        self.ep = 0\n",
        "    \n",
        "    def initialize_time_number(self):\n",
        "        self.t = 0\n",
        "    \n",
        "    def clear_batch_lists(self):\n",
        "        del self.saved_log_probs_s[:]\n",
        "        del self.saved_log_probs_n[:]\n",
        "        del self.saved_log_probs_o[:]\n",
        "        del self.reward_baselines[:]\n",
        "        del self.rewards[:]\n",
        "        del self.label_targets[:]\n",
        "        del self.label_predictions[:]\n",
        "        del self.state_values[:]\n",
        "        self.initialize_episode_number()\n",
        "        self.training_accuracy = 0.0\n",
        "    \n",
        "    def initialize_for_new_batch(self):\n",
        "        \"\"\"\n",
        "        Cleans history of log probabilities, rewards, targets etc for the last\n",
        "        batch\n",
        "        \"\"\"\n",
        "        self.initialize_episode_number()\n",
        "        \n",
        "        # Episode policy and reward history \n",
        "        self.saved_log_probs_s = [] # log probabilities for each time step t in each episode in batch\n",
        "        self.saved_log_probs_n = [] # log probs for jump\n",
        "        self.saved_log_probs_o = [] # log_prob for class\n",
        "        self.rewards = [] # reward at final time step of each episode in batch\n",
        "        self.reward_baselines = [] # reward baselines for each time step t in each episode in batch\n",
        "        self.state_values = []\n",
        "\n",
        "        # Predictions and targets history (for cross entropy loss calculation)\n",
        "        self.label_predictions = [] # 1 probability for each episode\n",
        "        self.label_targets = []# 1 label for each episode\n",
        "        self.training_accuracy = 0.0\n",
        "\n",
        "        \n",
        "    def classify(self):\n",
        "        # global temp\n",
        "        # temp.append(self.c[0])\n",
        "        out = self.output_linear_1(self.c[0])\n",
        "        out = self.output_linear_2(out)\n",
        "        self.label_predictions.append(out)\n",
        "        prob_o = torch.sigmoid(out)\n",
        "        class_categ = Binomial(probs=prob_o)\n",
        "        _class = class_categ.sample()\n",
        "        if self.train_mode:\n",
        "            self.rewards.append(reward_function(_class, self.label_targets[-1]))\n",
        "            self.saved_log_probs_o.append((class_categ.log_prob(_class), ))\n",
        "        # return torch.sigmoid(out)\n",
        "       \n",
        "    def get_baseline(self):\n",
        "        return torch.dot(self.wb, self.c[0].detach()) + self.cb\n",
        "    \n",
        "    def save_training_accuracy(self):\n",
        "        correct = 0\n",
        "        for _r in self.rewards:\n",
        "            if _r > 0:\n",
        "                correct += 1\n",
        "        self.training_accuracy = correct/len(self.rewards)\n",
        "        self.training_accuracies.append(self.training_accuracy)\n",
        "      \n",
        "    \n",
        "    def forward(self, pack):\n",
        "        texts, lengths, labels = pack\n",
        "        embeddeds = self.embedding(texts)\n",
        "        # embeddeds = nn.utils.rnn.pack_padded_sequence(embeddeds, lengths)\n",
        "        self.initialize_for_new_batch()\n",
        "\n",
        "        for episode_number in range(embeddeds.shape[1]):\n",
        "            \n",
        "            # load episode data\n",
        "            self.ep = episode_number\n",
        "            embedded = embeddeds[:, self.ep, :]\n",
        "      \n",
        "            #print(texts.shape, embeddeds.shape, embedded.shape)\n",
        "            #print(label)\n",
        "            \n",
        "            # initialize counters and index\n",
        "            tokens_read = 0\n",
        "            jumps_made = 0\n",
        "            word_index = 0\n",
        "            words_len = embedded.shape[0]\n",
        "            self.initialize_lstm_cell_states()\n",
        "            self.initialize_time_number()\n",
        "            self.saved_log_probs_s.append([])\n",
        "            self.saved_log_probs_n.append([])\n",
        "            self.state_values.append([])\n",
        "            self.reward_baselines.append([])            \n",
        "            if self.train_mode:\n",
        "                label = labels[self.ep].reshape(1)\n",
        "                self.label_targets.append(label)\n",
        "\n",
        "            # start iterating through sequence, while skipping some words\n",
        "            while word_index<words_len and word_index<400:\n",
        "                self.t += 1                \n",
        "                #print(\"embedded_word\", embedded_word.shape)\n",
        "\n",
        "                # generate next lstm cell state\n",
        "                for _r in range(min(R, words_len-word_index)):\n",
        "                    embedded_word = embedded[word_index]\n",
        "                    self.h, self.c = self.lstm_cell(torch.reshape(embedded_word, (1, -1)), (self.h, self.c))\n",
        "                    word_index += 1\n",
        "                # print('word_index', word_index, 'tokens_read', tokens_read, 'jumps_made', jumps_made)\n",
        "                \n",
        "                # print(self.c)\n",
        "                _state_value = self.value_head(self.c[0])\n",
        "                self.state_values.append(_state_value)\n",
        "                \n",
        "                _s = self.stop_linear_1(self.c[0])\n",
        "                _s = F.relu(_s)\n",
        "                _s = self.stop_linear_2(_s)\n",
        "                _s = F.relu(_s)\n",
        "                _s = self.stop_linear_3(_s)\n",
        "\n",
        "                probs_s = torch.sigmoid(_s)\n",
        "                try:\n",
        "                    stop_categ = Binomial(probs=probs_s)\n",
        "                    stop = stop_categ.sample()\n",
        "                except:\n",
        "                    print(\"_c\", self.c)\n",
        "                    #temp = (self.c, _s, probs_s, self.stop_linear_1, self.stop_linear_2, self.stop_linear_3, stop_categ)\n",
        "                    raise ValueError('got the expected error')\n",
        "              # Add log probability of our chosen action to our history\n",
        "                self.saved_log_probs_s[-1].append(stop_categ.log_prob(stop))\n",
        "                self.reward_baselines[-1].append(self.get_baseline())\n",
        "\n",
        "                if stop > 0.5:\n",
        "                    self.classify()\n",
        "                    break\n",
        "                else:\n",
        "                    _n = self.jumping_linear_1(self.c[0])\n",
        "                    _n = F.relu(_n)\n",
        "                    _n = self.jumping_linear_2(_n)\n",
        "                    _n = F.relu(_n)\n",
        "                    _n = self.jumping_linear_3(_n)\n",
        "                    _n = F.softmax(_n)\n",
        "                    n_categ = Categorical(_n)\n",
        "                    n = n_categ.sample()\n",
        "                    self.saved_log_probs_n[-1].append(n_categ.log_prob(n))\n",
        "                    word_index += n * R\n",
        "            else:\n",
        "                # print(\"Finished while loop\")\n",
        "                # raise ValueError('Finshed ')\n",
        "                self.classify()\n",
        "        if self.train_mode:\n",
        "            self.save_training_accuracy()\n",
        "        return self.label_predictions \n",
        "      "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ZyrocBqSF98w",
        "colab": {}
      },
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "EMBEDDING_DIM = 100\n",
        "HIDDEN_DIM = 128\n",
        "OUTPUT_DIM = 1\n",
        "FLOP_COST = 0.0001\n",
        "\n",
        "seed = 7\n",
        "torch.manual_seed(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "np.random.seed(seed)\n",
        "\n",
        "bce = nn.BCEWithLogitsLoss(reduction='mean')\n",
        "policy_model = SkipReadingModel(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM).to(device)\n",
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "policy_model.embedding.weight.data.copy_(pretrained_embeddings)\n",
        "\n",
        "if os.path.exists('fast_text_model.weights'):\n",
        "    policy_model.load_state_dict(torch.load('fast_text_model.weights'))\n",
        "\n",
        "# define the optimiser\n",
        "optimizer = optim.Adam(policy_model.parameters(), lr=0.01)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P9A3CLqzugLZ",
        "colab": {}
      },
      "source": [
        "def update_policy():\n",
        "   \n",
        "    #print(len(policy_model.rewards), len(policy_model.label_predictions), len(policy_model.label_targets))\n",
        "    #print(len(policy_model.saved_log_probs_o), len(policy_model.saved_log_probs_n), len(policy_model.saved_log_probs_s))\n",
        "    #print(len(policy_model.reward_baselines))\n",
        "    \n",
        "    policy_loss_sum = torch.tensor(0.0, requires_grad=True)\n",
        "    reward_sum = torch.tensor(0.0, requires_grad=True)\n",
        "    baseline_loss_sum = torch.tensor(0.0, requires_grad=True)\n",
        "    value_loss_sum = torch.tensor(0.0, requires_grad=True)\n",
        "    \n",
        "    for reward, prediction, target, log_probs_o, log_probs_n, log_probs_s, baselines, svs in zip(\n",
        "        policy_model.rewards, policy_model.label_predictions,\n",
        "        policy_model.label_targets, policy_model.saved_log_probs_o,\n",
        "        policy_model.saved_log_probs_n, policy_model.saved_log_probs_s,\n",
        "        policy_model.reward_baselines, policy_model.state_values):\n",
        "      \n",
        "            for lpn in log_probs_n:\n",
        "                policy_loss_sum = policy_loss_sum + lpn\n",
        "          \n",
        "            for i, (lps, b, sv) in enumerate(zip(log_probs_s, baselines, svs)):\n",
        "                policy_loss_sum = policy_loss_sum + lps\n",
        "                r = torch.pow(policy_model.gamma, i) * (-FLOP_COST)\n",
        "                if i == len(svs)-1:\n",
        "                    r = r + torch.pow(policy_model.gamma, i) * reward\n",
        "                adv = r - sv.item()\n",
        "                reward_sum = reward_sum + adv\n",
        "                value_loss_sum = value_loss_sum + F.smooth_l1_loss(sv, torch.tensor([r]))\n",
        "                # baseline_loss_sum = baseline_loss_sum + torch.pow(rew, 2)\n",
        "\n",
        "            # baseline_loss_sum = baseline_loss_sum - torch.pow(rew, 2) + torch.pow(torch.pow(policy_model.gamma, i) * reward + rew, 2)\n",
        "            policy_loss_sum = policy_loss_sum + log_probs_o[0]\n",
        "    \n",
        "    # print(\"reward sum\", reward_sum)\n",
        "    # print(\"policy_loss_sum\", policy_loss_sum)\n",
        "    \n",
        "    loss = policy_loss_sum * reward_sum + value_loss_sum\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    # print('policy_loss', policy_loss)\n",
        "    loss.backward(retain_graph=True)\n",
        "    optimizer.step()\n",
        "    \n",
        "    policy_model.clear_batch_lists()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "B-mGeEeZZgRf",
        "colab": {}
      },
      "source": [
        "def test_model():\n",
        "    policy_model.train_mode = True\n",
        "    correct = 0\n",
        "    total=0\n",
        "    for _data in test_iterator:\n",
        "        # get the inputs\n",
        "        texts, text_lengths, labels = _data.text[0], _data.text[1], _data.label\n",
        "        # print(\"Input review texts, text_lengths, labels\", texts.shape, text_lengths.shape, labels.shape)\n",
        "        predictions = policy_model((texts.to(device), text_lengths.to(device), labels.to(device)))\n",
        "        for (prediction, label) in zip(predictions, labels):\n",
        "            if reward_function(label, prediction) > 0:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "        if total%1000 == 0:\n",
        "            print(total)\n",
        "        if total%5000 == 0:\n",
        "            break\n",
        "    print(\"Test accuracy :\", correct/total)\n",
        "    policy_model.train_mode = True\n",
        "    return correct/total\n",
        "\n",
        "def validate_model():\n",
        "    policy_model.train_mode = True\n",
        "    correct = 0\n",
        "    total=0\n",
        "    for _data in valid_iterator:\n",
        "        # get the inputs\n",
        "        texts, text_lengths, labels = _data.text[0], _data.text[1], _data.label\n",
        "        # print(\"Input review texts, text_lengths, labels\", texts.shape, text_lengths.shape, labels.shape)\n",
        "        predictions = policy_model((texts.to(device), text_lengths.to(device), labels.to(device)))\n",
        "        for (prediction, label) in zip(predictions, labels):\n",
        "            if reward_function(label, prediction) > 0:\n",
        "                correct += 1\n",
        "            total += 1\n",
        "        if total%1000 == 0:\n",
        "            break\n",
        "    print(\"Validation accuracy :\", correct/total)\n",
        "    policy_model.train_mode = True\n",
        "    policy_model.validation_accuracies.append(correct/total)\n",
        "    return correct/total\n",
        "\n",
        "# test_model()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "grN0kN4pL1BX",
        "outputId": "21126e0e-396e-40b1-8093-e9c29f0d22d3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 13293
        }
      },
      "source": [
        "# the epoch loop\n",
        "\n",
        "with torch.enable_grad():\n",
        "    validate_model()\n",
        "    for epoch in range(10):\n",
        "        running_reward = 10\n",
        "        t = 0\n",
        "        for _data in train_iterator:\n",
        "            # get the inputs\n",
        "            texts, text_lengths, labels = _data.text[0], _data.text[1], _data.label\n",
        "            # print(\"Input review texts, text_lengths, labels\", texts.shape, text_lengths.shape, labels.shape)\n",
        "            prediction = policy_model((texts.to(device), text_lengths.to(device), labels.to(device)))\n",
        "            # print(\"Prediction\", prediction.item())        \n",
        "            # raise ValueError('Not done')\n",
        "            t += 1\n",
        "            if t%2 == 0:\n",
        "                print(\"batch no. %d, training accuracy %4.2f\" % (t, policy_model.training_accuracy))\n",
        "            if t%10 == 0:\n",
        "                validate_model()\n",
        "                #print(\"wb\", policy_model.wb)\n",
        "                # print(\"lstm hh hi\", policy_model.lstm_cell.weight_hh[0][::10], policy_model.lstm_cell.weight_ih[0][::10])\n",
        "                #print(\"lstm hh hi\", policy_model.lstm_cell.weight_hh, policy_model.lstm_cell.weight_ih)\n",
        "                #print(\"emb\", policy_model.embedding.weight)\n",
        "                #print(\"jmp\", policy_model.jumping_linear.weight)\n",
        "                #print(\"out\", policy_model.output_linear.weight)\n",
        "            if t%1000 == 0:\n",
        "                break\n",
        "            update_policy()\n",
        "        #running_reward = 0.05 * policy_model.reward_episode + (1 - 0.05) * running_reward\n",
        "        #print(\"Epoch %d, reward %4.2f\" % (epoch, running_reward))\n",
        "        print(\"Epoch %d\" % (epoch))\n",
        "    print('**** Finished Training ****')\n",
        "# test_model()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:210: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Validation accuracy : 0.424\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:28: UserWarning: Using a target size (torch.Size([1])) that is different to the input size (torch.Size([])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "batch no. 2, training accuracy 0.54\n",
            "batch no. 4, training accuracy 0.52\n",
            "batch no. 6, training accuracy 0.52\n",
            "batch no. 8, training accuracy 0.44\n",
            "batch no. 10, training accuracy 0.56\n",
            "Validation accuracy : 0.576\n",
            "batch no. 12, training accuracy 0.48\n",
            "batch no. 14, training accuracy 0.66\n",
            "batch no. 16, training accuracy 0.58\n",
            "batch no. 18, training accuracy 0.46\n",
            "batch no. 20, training accuracy 0.46\n",
            "Validation accuracy : 0.576\n",
            "batch no. 22, training accuracy 0.46\n",
            "batch no. 24, training accuracy 0.46\n",
            "batch no. 26, training accuracy 0.46\n",
            "batch no. 28, training accuracy 0.44\n",
            "batch no. 30, training accuracy 0.44\n",
            "Validation accuracy : 0.576\n",
            "batch no. 32, training accuracy 0.42\n",
            "batch no. 34, training accuracy 0.54\n",
            "batch no. 36, training accuracy 0.56\n",
            "batch no. 38, training accuracy 0.52\n",
            "batch no. 40, training accuracy 0.42\n",
            "Validation accuracy : 0.576\n",
            "batch no. 42, training accuracy 0.62\n",
            "batch no. 44, training accuracy 0.48\n",
            "batch no. 46, training accuracy 0.42\n",
            "batch no. 48, training accuracy 0.46\n",
            "batch no. 50, training accuracy 0.52\n",
            "Validation accuracy : 0.576\n",
            "batch no. 52, training accuracy 0.56\n",
            "batch no. 54, training accuracy 0.56\n",
            "batch no. 56, training accuracy 0.46\n",
            "batch no. 58, training accuracy 0.54\n",
            "batch no. 60, training accuracy 0.48\n",
            "Validation accuracy : 0.576\n",
            "batch no. 62, training accuracy 0.48\n",
            "batch no. 64, training accuracy 0.48\n",
            "batch no. 66, training accuracy 0.52\n",
            "batch no. 68, training accuracy 0.48\n",
            "batch no. 70, training accuracy 0.50\n",
            "Validation accuracy : 0.576\n",
            "batch no. 72, training accuracy 0.40\n",
            "batch no. 74, training accuracy 0.62\n",
            "batch no. 76, training accuracy 0.54\n",
            "batch no. 78, training accuracy 0.46\n",
            "batch no. 80, training accuracy 0.50\n",
            "Validation accuracy : 0.576\n",
            "batch no. 82, training accuracy 0.52\n",
            "batch no. 84, training accuracy 0.52\n",
            "batch no. 86, training accuracy 0.54\n",
            "batch no. 88, training accuracy 0.52\n",
            "batch no. 90, training accuracy 0.50\n",
            "Validation accuracy : 0.576\n",
            "batch no. 92, training accuracy 0.52\n",
            "batch no. 94, training accuracy 0.54\n",
            "batch no. 96, training accuracy 0.64\n",
            "batch no. 98, training accuracy 0.50\n",
            "batch no. 100, training accuracy 0.58\n",
            "Validation accuracy : 0.576\n",
            "batch no. 102, training accuracy 0.56\n",
            "batch no. 104, training accuracy 0.58\n",
            "batch no. 106, training accuracy 0.54\n",
            "batch no. 108, training accuracy 0.58\n",
            "batch no. 110, training accuracy 0.54\n",
            "Validation accuracy : 0.576\n",
            "batch no. 112, training accuracy 0.50\n",
            "batch no. 114, training accuracy 0.54\n",
            "batch no. 116, training accuracy 0.62\n",
            "batch no. 118, training accuracy 0.40\n",
            "batch no. 120, training accuracy 0.54\n",
            "Validation accuracy : 0.576\n",
            "batch no. 122, training accuracy 0.64\n",
            "batch no. 124, training accuracy 0.40\n",
            "batch no. 126, training accuracy 0.52\n",
            "batch no. 128, training accuracy 0.52\n",
            "batch no. 130, training accuracy 0.36\n",
            "Validation accuracy : 0.576\n",
            "batch no. 132, training accuracy 0.42\n",
            "batch no. 134, training accuracy 0.52\n",
            "batch no. 136, training accuracy 0.56\n",
            "batch no. 138, training accuracy 0.44\n",
            "batch no. 140, training accuracy 0.48\n",
            "Validation accuracy : 0.576\n",
            "batch no. 142, training accuracy 0.58\n",
            "batch no. 144, training accuracy 0.46\n",
            "batch no. 146, training accuracy 0.56\n",
            "batch no. 148, training accuracy 0.54\n",
            "batch no. 150, training accuracy 0.44\n",
            "Validation accuracy : 0.576\n",
            "batch no. 152, training accuracy 0.54\n",
            "batch no. 154, training accuracy 0.44\n",
            "batch no. 156, training accuracy 0.44\n",
            "batch no. 158, training accuracy 0.44\n",
            "batch no. 160, training accuracy 0.62\n",
            "Validation accuracy : 0.576\n",
            "batch no. 162, training accuracy 0.38\n",
            "batch no. 164, training accuracy 0.50\n",
            "batch no. 166, training accuracy 0.50\n",
            "batch no. 168, training accuracy 0.52\n",
            "batch no. 170, training accuracy 0.62\n",
            "Validation accuracy : 0.576\n",
            "batch no. 172, training accuracy 0.44\n",
            "batch no. 174, training accuracy 0.52\n",
            "batch no. 176, training accuracy 0.44\n",
            "batch no. 178, training accuracy 0.48\n",
            "batch no. 180, training accuracy 0.58\n",
            "Validation accuracy : 0.576\n",
            "batch no. 182, training accuracy 0.44\n",
            "batch no. 184, training accuracy 0.52\n",
            "batch no. 186, training accuracy 0.28\n",
            "batch no. 188, training accuracy 0.48\n",
            "batch no. 190, training accuracy 0.58\n",
            "Validation accuracy : 0.576\n",
            "batch no. 192, training accuracy 0.56\n",
            "batch no. 194, training accuracy 0.62\n",
            "batch no. 196, training accuracy 0.60\n",
            "batch no. 198, training accuracy 0.60\n",
            "batch no. 200, training accuracy 0.48\n",
            "Validation accuracy : 0.576\n",
            "batch no. 202, training accuracy 0.60\n",
            "batch no. 204, training accuracy 0.72\n",
            "batch no. 206, training accuracy 0.54\n",
            "batch no. 208, training accuracy 0.60\n",
            "batch no. 210, training accuracy 0.68\n",
            "Validation accuracy : 0.576\n",
            "batch no. 212, training accuracy 0.40\n",
            "batch no. 214, training accuracy 0.54\n",
            "batch no. 216, training accuracy 0.58\n",
            "batch no. 218, training accuracy 0.48\n",
            "batch no. 220, training accuracy 0.46\n",
            "Validation accuracy : 0.576\n",
            "batch no. 222, training accuracy 0.40\n",
            "batch no. 224, training accuracy 0.54\n",
            "batch no. 226, training accuracy 0.58\n",
            "batch no. 228, training accuracy 0.46\n",
            "batch no. 230, training accuracy 0.36\n",
            "Validation accuracy : 0.576\n",
            "batch no. 232, training accuracy 0.48\n",
            "batch no. 234, training accuracy 0.44\n",
            "batch no. 236, training accuracy 0.46\n",
            "batch no. 238, training accuracy 0.52\n",
            "batch no. 240, training accuracy 0.52\n",
            "Validation accuracy : 0.576\n",
            "batch no. 242, training accuracy 0.54\n",
            "batch no. 244, training accuracy 0.42\n",
            "batch no. 246, training accuracy 0.36\n",
            "batch no. 248, training accuracy 0.44\n",
            "batch no. 250, training accuracy 0.46\n",
            "Validation accuracy : 0.576\n",
            "batch no. 252, training accuracy 0.44\n",
            "batch no. 254, training accuracy 0.50\n",
            "batch no. 256, training accuracy 0.44\n",
            "batch no. 258, training accuracy 0.54\n",
            "batch no. 260, training accuracy 0.38\n",
            "Validation accuracy : 0.576\n",
            "batch no. 262, training accuracy 0.58\n",
            "batch no. 264, training accuracy 0.36\n",
            "batch no. 266, training accuracy 0.46\n",
            "batch no. 268, training accuracy 0.46\n",
            "batch no. 270, training accuracy 0.44\n",
            "Validation accuracy : 0.576\n",
            "batch no. 272, training accuracy 0.56\n",
            "batch no. 274, training accuracy 0.44\n",
            "batch no. 276, training accuracy 0.50\n",
            "batch no. 278, training accuracy 0.60\n",
            "batch no. 280, training accuracy 0.52\n",
            "Validation accuracy : 0.576\n",
            "batch no. 282, training accuracy 0.62\n",
            "batch no. 284, training accuracy 0.56\n",
            "batch no. 286, training accuracy 0.60\n",
            "batch no. 288, training accuracy 0.52\n",
            "batch no. 290, training accuracy 0.42\n",
            "Validation accuracy : 0.576\n",
            "batch no. 292, training accuracy 0.44\n",
            "batch no. 294, training accuracy 0.54\n",
            "batch no. 296, training accuracy 0.44\n",
            "batch no. 298, training accuracy 0.54\n",
            "batch no. 300, training accuracy 0.62\n",
            "Validation accuracy : 0.576\n",
            "batch no. 302, training accuracy 0.56\n",
            "batch no. 304, training accuracy 0.58\n",
            "batch no. 306, training accuracy 0.54\n",
            "batch no. 308, training accuracy 0.66\n",
            "batch no. 310, training accuracy 0.48\n",
            "Validation accuracy : 0.576\n",
            "batch no. 312, training accuracy 0.60\n",
            "batch no. 314, training accuracy 0.52\n",
            "batch no. 316, training accuracy 0.40\n",
            "batch no. 318, training accuracy 0.40\n",
            "batch no. 320, training accuracy 0.54\n",
            "Validation accuracy : 0.576\n",
            "batch no. 322, training accuracy 0.40\n",
            "batch no. 324, training accuracy 0.50\n",
            "batch no. 326, training accuracy 0.60\n",
            "batch no. 328, training accuracy 0.56\n",
            "batch no. 330, training accuracy 0.48\n",
            "Validation accuracy : 0.576\n",
            "batch no. 332, training accuracy 0.58\n",
            "batch no. 334, training accuracy 0.50\n",
            "batch no. 336, training accuracy 0.52\n",
            "batch no. 338, training accuracy 0.56\n",
            "batch no. 340, training accuracy 0.46\n",
            "Validation accuracy : 0.576\n",
            "batch no. 342, training accuracy 0.52\n",
            "batch no. 344, training accuracy 0.36\n",
            "batch no. 346, training accuracy 0.52\n",
            "batch no. 348, training accuracy 0.44\n",
            "batch no. 350, training accuracy 0.40\n",
            "Validation accuracy : 0.576\n",
            "batch no. 352, training accuracy 0.52\n",
            "batch no. 354, training accuracy 0.54\n",
            "batch no. 356, training accuracy 0.56\n",
            "batch no. 358, training accuracy 0.48\n",
            "batch no. 360, training accuracy 0.46\n",
            "Validation accuracy : 0.576\n",
            "batch no. 362, training accuracy 0.48\n",
            "batch no. 364, training accuracy 0.50\n",
            "batch no. 366, training accuracy 0.56\n",
            "batch no. 368, training accuracy 0.54\n",
            "batch no. 370, training accuracy 0.54\n",
            "Validation accuracy : 0.576\n",
            "batch no. 372, training accuracy 0.40\n",
            "batch no. 374, training accuracy 0.48\n",
            "batch no. 376, training accuracy 0.54\n",
            "batch no. 378, training accuracy 0.56\n",
            "batch no. 380, training accuracy 0.60\n",
            "Validation accuracy : 0.576\n",
            "batch no. 382, training accuracy 0.42\n",
            "batch no. 384, training accuracy 0.38\n",
            "batch no. 386, training accuracy 0.36\n",
            "batch no. 388, training accuracy 0.52\n",
            "batch no. 390, training accuracy 0.52\n",
            "Validation accuracy : 0.576\n",
            "batch no. 392, training accuracy 0.58\n",
            "batch no. 394, training accuracy 0.56\n",
            "batch no. 396, training accuracy 0.60\n",
            "batch no. 398, training accuracy 0.46\n",
            "batch no. 400, training accuracy 0.52\n",
            "Validation accuracy : 0.576\n",
            "Epoch 0\n",
            "batch no. 2, training accuracy 0.60\n",
            "batch no. 4, training accuracy 0.54\n",
            "batch no. 6, training accuracy 0.52\n",
            "batch no. 8, training accuracy 0.44\n",
            "batch no. 10, training accuracy 0.56\n",
            "Validation accuracy : 0.576\n",
            "batch no. 12, training accuracy 0.48\n",
            "batch no. 14, training accuracy 0.66\n",
            "batch no. 16, training accuracy 0.58\n",
            "batch no. 18, training accuracy 0.46\n",
            "batch no. 20, training accuracy 0.46\n",
            "Validation accuracy : 0.576\n",
            "batch no. 22, training accuracy 0.46\n",
            "batch no. 24, training accuracy 0.46\n",
            "batch no. 26, training accuracy 0.46\n",
            "batch no. 28, training accuracy 0.44\n",
            "batch no. 30, training accuracy 0.44\n",
            "Validation accuracy : 0.576\n",
            "batch no. 32, training accuracy 0.42\n",
            "batch no. 34, training accuracy 0.54\n",
            "batch no. 36, training accuracy 0.56\n",
            "batch no. 38, training accuracy 0.52\n",
            "batch no. 40, training accuracy 0.42\n",
            "Validation accuracy : 0.576\n",
            "batch no. 42, training accuracy 0.62\n",
            "batch no. 44, training accuracy 0.48\n",
            "batch no. 46, training accuracy 0.42\n",
            "batch no. 48, training accuracy 0.46\n",
            "batch no. 50, training accuracy 0.52\n",
            "Validation accuracy : 0.576\n",
            "batch no. 52, training accuracy 0.56\n",
            "batch no. 54, training accuracy 0.56\n",
            "batch no. 56, training accuracy 0.46\n",
            "batch no. 58, training accuracy 0.54\n",
            "batch no. 60, training accuracy 0.50\n",
            "Validation accuracy : 0.556\n",
            "batch no. 62, training accuracy 0.50\n",
            "batch no. 64, training accuracy 0.46\n",
            "batch no. 66, training accuracy 0.52\n",
            "batch no. 68, training accuracy 0.48\n",
            "batch no. 70, training accuracy 0.50\n",
            "Validation accuracy : 0.558\n",
            "batch no. 72, training accuracy 0.42\n",
            "batch no. 74, training accuracy 0.60\n",
            "batch no. 76, training accuracy 0.54\n",
            "batch no. 78, training accuracy 0.46\n",
            "batch no. 80, training accuracy 0.50\n",
            "Validation accuracy : 0.566\n",
            "batch no. 82, training accuracy 0.52\n",
            "batch no. 84, training accuracy 0.52\n",
            "batch no. 86, training accuracy 0.54\n",
            "batch no. 88, training accuracy 0.50\n",
            "batch no. 90, training accuracy 0.52\n",
            "Validation accuracy : 0.565\n",
            "batch no. 92, training accuracy 0.52\n",
            "batch no. 94, training accuracy 0.54\n",
            "batch no. 96, training accuracy 0.64\n",
            "batch no. 98, training accuracy 0.50\n",
            "batch no. 100, training accuracy 0.56\n",
            "Validation accuracy : 0.56\n",
            "batch no. 102, training accuracy 0.56\n",
            "batch no. 104, training accuracy 0.58\n",
            "batch no. 106, training accuracy 0.54\n",
            "batch no. 108, training accuracy 0.56\n",
            "batch no. 110, training accuracy 0.54\n",
            "Validation accuracy : 0.564\n",
            "batch no. 112, training accuracy 0.50\n",
            "batch no. 114, training accuracy 0.52\n",
            "batch no. 116, training accuracy 0.64\n",
            "batch no. 118, training accuracy 0.40\n",
            "batch no. 120, training accuracy 0.54\n",
            "Validation accuracy : 0.565\n",
            "batch no. 122, training accuracy 0.66\n",
            "batch no. 124, training accuracy 0.40\n",
            "batch no. 126, training accuracy 0.50\n",
            "batch no. 128, training accuracy 0.52\n",
            "batch no. 130, training accuracy 0.38\n",
            "Validation accuracy : 0.568\n",
            "batch no. 132, training accuracy 0.42\n",
            "batch no. 134, training accuracy 0.48\n",
            "batch no. 136, training accuracy 0.56\n",
            "batch no. 138, training accuracy 0.38\n",
            "batch no. 140, training accuracy 0.52\n",
            "Validation accuracy : 0.569\n",
            "batch no. 142, training accuracy 0.56\n",
            "batch no. 144, training accuracy 0.48\n",
            "batch no. 146, training accuracy 0.46\n",
            "batch no. 148, training accuracy 0.48\n",
            "batch no. 150, training accuracy 0.54\n",
            "Validation accuracy : 0.459\n",
            "batch no. 152, training accuracy 0.46\n",
            "batch no. 154, training accuracy 0.58\n",
            "batch no. 156, training accuracy 0.56\n",
            "batch no. 158, training accuracy 0.58\n",
            "batch no. 160, training accuracy 0.38\n",
            "Validation accuracy : 0.426\n",
            "batch no. 162, training accuracy 0.62\n",
            "batch no. 164, training accuracy 0.50\n",
            "batch no. 166, training accuracy 0.50\n",
            "batch no. 168, training accuracy 0.48\n",
            "batch no. 170, training accuracy 0.38\n",
            "Validation accuracy : 0.421\n",
            "batch no. 172, training accuracy 0.54\n",
            "batch no. 174, training accuracy 0.48\n",
            "batch no. 176, training accuracy 0.60\n",
            "batch no. 178, training accuracy 0.44\n",
            "batch no. 180, training accuracy 0.56\n",
            "Validation accuracy : 0.504\n",
            "batch no. 182, training accuracy 0.58\n",
            "batch no. 184, training accuracy 0.46\n",
            "batch no. 186, training accuracy 0.70\n",
            "batch no. 188, training accuracy 0.52\n",
            "batch no. 190, training accuracy 0.42\n",
            "Validation accuracy : 0.504\n",
            "batch no. 192, training accuracy 0.50\n",
            "batch no. 194, training accuracy 0.38\n",
            "batch no. 196, training accuracy 0.40\n",
            "batch no. 198, training accuracy 0.40\n",
            "batch no. 200, training accuracy 0.52\n",
            "Validation accuracy : 0.505\n",
            "batch no. 202, training accuracy 0.38\n",
            "batch no. 204, training accuracy 0.72\n",
            "batch no. 206, training accuracy 0.50\n",
            "batch no. 208, training accuracy 0.42\n",
            "batch no. 210, training accuracy 0.32\n",
            "Validation accuracy : 0.507\n",
            "batch no. 212, training accuracy 0.60\n",
            "batch no. 214, training accuracy 0.54\n",
            "batch no. 216, training accuracy 0.58\n",
            "batch no. 218, training accuracy 0.44\n",
            "batch no. 220, training accuracy 0.46\n",
            "Validation accuracy : 0.507\n",
            "batch no. 222, training accuracy 0.62\n",
            "batch no. 224, training accuracy 0.46\n",
            "batch no. 226, training accuracy 0.42\n",
            "batch no. 228, training accuracy 0.52\n",
            "batch no. 230, training accuracy 0.62\n",
            "Validation accuracy : 0.506\n",
            "batch no. 232, training accuracy 0.52\n",
            "batch no. 234, training accuracy 0.56\n",
            "batch no. 236, training accuracy 0.50\n",
            "batch no. 238, training accuracy 0.46\n",
            "batch no. 240, training accuracy 0.50\n",
            "Validation accuracy : 0.506\n",
            "batch no. 242, training accuracy 0.44\n",
            "batch no. 244, training accuracy 0.44\n",
            "batch no. 246, training accuracy 0.40\n",
            "batch no. 248, training accuracy 0.40\n",
            "batch no. 250, training accuracy 0.46\n",
            "Validation accuracy : 0.508\n",
            "batch no. 252, training accuracy 0.58\n",
            "batch no. 254, training accuracy 0.54\n",
            "batch no. 256, training accuracy 0.54\n",
            "batch no. 258, training accuracy 0.44\n",
            "batch no. 260, training accuracy 0.60\n",
            "Validation accuracy : 0.507\n",
            "batch no. 262, training accuracy 0.54\n",
            "batch no. 264, training accuracy 0.38\n",
            "batch no. 266, training accuracy 0.54\n",
            "batch no. 268, training accuracy 0.52\n",
            "batch no. 270, training accuracy 0.52\n",
            "Validation accuracy : 0.507\n",
            "batch no. 272, training accuracy 0.58\n",
            "batch no. 274, training accuracy 0.42\n",
            "batch no. 276, training accuracy 0.50\n",
            "batch no. 278, training accuracy 0.48\n",
            "batch no. 280, training accuracy 0.56\n",
            "Validation accuracy : 0.506\n",
            "batch no. 282, training accuracy 0.42\n",
            "batch no. 284, training accuracy 0.46\n",
            "batch no. 286, training accuracy 0.40\n",
            "batch no. 288, training accuracy 0.46\n",
            "batch no. 290, training accuracy 0.58\n",
            "Validation accuracy : 0.503\n",
            "batch no. 292, training accuracy 0.54\n",
            "batch no. 294, training accuracy 0.48\n",
            "batch no. 296, training accuracy 0.52\n",
            "batch no. 298, training accuracy 0.48\n",
            "batch no. 300, training accuracy 0.38\n",
            "Validation accuracy : 0.503\n",
            "batch no. 302, training accuracy 0.44\n",
            "batch no. 304, training accuracy 0.64\n",
            "batch no. 306, training accuracy 0.50\n",
            "batch no. 308, training accuracy 0.38\n",
            "batch no. 310, training accuracy 0.52\n",
            "Validation accuracy : 0.505\n",
            "batch no. 312, training accuracy 0.42\n",
            "batch no. 314, training accuracy 0.48\n",
            "batch no. 316, training accuracy 0.34\n",
            "batch no. 318, training accuracy 0.44\n",
            "batch no. 320, training accuracy 0.40\n",
            "Validation accuracy : 0.502\n",
            "batch no. 322, training accuracy 0.44\n",
            "batch no. 324, training accuracy 0.46\n",
            "batch no. 326, training accuracy 0.40\n",
            "batch no. 328, training accuracy 0.54\n",
            "batch no. 330, training accuracy 0.52\n",
            "Validation accuracy : 0.503\n",
            "batch no. 332, training accuracy 0.36\n",
            "batch no. 334, training accuracy 0.52\n",
            "batch no. 336, training accuracy 0.52\n",
            "batch no. 338, training accuracy 0.44\n",
            "batch no. 340, training accuracy 0.54\n",
            "Validation accuracy : 0.502\n",
            "batch no. 342, training accuracy 0.52\n",
            "batch no. 344, training accuracy 0.42\n",
            "batch no. 346, training accuracy 0.50\n",
            "batch no. 348, training accuracy 0.44\n",
            "batch no. 350, training accuracy 0.58\n",
            "Validation accuracy : 0.502\n",
            "batch no. 352, training accuracy 0.46\n",
            "batch no. 354, training accuracy 0.48\n",
            "batch no. 356, training accuracy 0.46\n",
            "batch no. 358, training accuracy 0.50\n",
            "batch no. 360, training accuracy 0.48\n",
            "Validation accuracy : 0.505\n",
            "batch no. 362, training accuracy 0.40\n",
            "batch no. 364, training accuracy 0.56\n",
            "batch no. 366, training accuracy 0.42\n",
            "batch no. 368, training accuracy 0.44\n",
            "batch no. 370, training accuracy 0.44\n",
            "Validation accuracy : 0.504\n",
            "batch no. 372, training accuracy 0.44\n",
            "batch no. 374, training accuracy 0.48\n",
            "batch no. 376, training accuracy 0.44\n",
            "batch no. 378, training accuracy 0.44\n",
            "batch no. 380, training accuracy 0.56\n",
            "Validation accuracy : 0.503\n",
            "batch no. 382, training accuracy 0.60\n",
            "batch no. 384, training accuracy 0.58\n",
            "batch no. 386, training accuracy 0.62\n",
            "batch no. 388, training accuracy 0.44\n",
            "batch no. 390, training accuracy 0.48\n",
            "Validation accuracy : 0.503\n",
            "batch no. 392, training accuracy 0.44\n",
            "batch no. 394, training accuracy 0.42\n",
            "batch no. 396, training accuracy 0.42\n",
            "batch no. 398, training accuracy 0.56\n",
            "batch no. 400, training accuracy 0.50\n",
            "Validation accuracy : 0.502\n",
            "Epoch 1\n",
            "batch no. 2, training accuracy 0.38\n",
            "batch no. 4, training accuracy 0.54\n",
            "batch no. 6, training accuracy 0.50\n",
            "batch no. 8, training accuracy 0.56\n",
            "batch no. 10, training accuracy 0.48\n",
            "Validation accuracy : 0.502\n",
            "batch no. 12, training accuracy 0.56\n",
            "batch no. 14, training accuracy 0.60\n",
            "batch no. 16, training accuracy 0.50\n",
            "batch no. 18, training accuracy 0.50\n",
            "batch no. 20, training accuracy 0.50\n",
            "Validation accuracy : 0.501\n",
            "batch no. 22, training accuracy 0.56\n",
            "batch no. 24, training accuracy 0.54\n",
            "batch no. 26, training accuracy 0.56\n",
            "batch no. 28, training accuracy 0.58\n",
            "batch no. 30, training accuracy 0.56\n",
            "Validation accuracy : 0.504\n",
            "batch no. 32, training accuracy 0.58\n",
            "batch no. 34, training accuracy 0.48\n",
            "batch no. 36, training accuracy 0.42\n",
            "batch no. 38, training accuracy 0.38\n",
            "batch no. 40, training accuracy 0.56\n",
            "Validation accuracy : 0.502\n",
            "batch no. 42, training accuracy 0.62\n",
            "batch no. 44, training accuracy 0.52\n",
            "batch no. 46, training accuracy 0.44\n",
            "batch no. 48, training accuracy 0.48\n",
            "batch no. 50, training accuracy 0.46\n",
            "Validation accuracy : 0.5\n",
            "batch no. 52, training accuracy 0.48\n",
            "batch no. 54, training accuracy 0.46\n",
            "batch no. 56, training accuracy 0.52\n",
            "batch no. 58, training accuracy 0.48\n",
            "batch no. 60, training accuracy 0.52\n",
            "Validation accuracy : 0.5\n",
            "batch no. 62, training accuracy 0.38\n",
            "batch no. 64, training accuracy 0.48\n",
            "batch no. 66, training accuracy 0.48\n",
            "batch no. 68, training accuracy 0.50\n",
            "batch no. 70, training accuracy 0.50\n",
            "Validation accuracy : 0.499\n",
            "batch no. 72, training accuracy 0.44\n",
            "batch no. 74, training accuracy 0.38\n",
            "batch no. 76, training accuracy 0.50\n",
            "batch no. 78, training accuracy 0.40\n",
            "batch no. 80, training accuracy 0.48\n",
            "Validation accuracy : 0.499\n",
            "batch no. 82, training accuracy 0.46\n",
            "batch no. 84, training accuracy 0.48\n",
            "batch no. 86, training accuracy 0.46\n",
            "batch no. 88, training accuracy 0.46\n",
            "batch no. 90, training accuracy 0.52\n",
            "Validation accuracy : 0.499\n",
            "batch no. 92, training accuracy 0.48\n",
            "batch no. 94, training accuracy 0.44\n",
            "batch no. 96, training accuracy 0.38\n",
            "batch no. 98, training accuracy 0.52\n",
            "batch no. 100, training accuracy 0.40\n",
            "Validation accuracy : 0.5\n",
            "batch no. 102, training accuracy 0.42\n",
            "batch no. 104, training accuracy 0.50\n",
            "batch no. 106, training accuracy 0.56\n",
            "batch no. 108, training accuracy 0.42\n",
            "batch no. 110, training accuracy 0.46\n",
            "Validation accuracy : 0.5\n",
            "batch no. 112, training accuracy 0.50\n",
            "batch no. 114, training accuracy 0.50\n",
            "batch no. 116, training accuracy 0.52\n",
            "batch no. 118, training accuracy 0.46\n",
            "batch no. 120, training accuracy 0.56\n",
            "Validation accuracy : 0.501\n",
            "batch no. 122, training accuracy 0.34\n",
            "batch no. 124, training accuracy 0.60\n",
            "batch no. 126, training accuracy 0.50\n",
            "batch no. 128, training accuracy 0.48\n",
            "batch no. 130, training accuracy 0.64\n",
            "Validation accuracy : 0.5\n",
            "batch no. 132, training accuracy 0.58\n",
            "batch no. 134, training accuracy 0.48\n",
            "batch no. 136, training accuracy 0.44\n",
            "batch no. 138, training accuracy 0.56\n",
            "batch no. 140, training accuracy 0.52\n",
            "Validation accuracy : 0.5\n",
            "batch no. 142, training accuracy 0.44\n",
            "batch no. 144, training accuracy 0.42\n",
            "batch no. 146, training accuracy 0.52\n",
            "batch no. 148, training accuracy 0.56\n",
            "batch no. 150, training accuracy 0.46\n",
            "Validation accuracy : 0.5\n",
            "batch no. 152, training accuracy 0.46\n",
            "batch no. 154, training accuracy 0.54\n",
            "batch no. 156, training accuracy 0.54\n",
            "batch no. 158, training accuracy 0.58\n",
            "batch no. 160, training accuracy 0.38\n",
            "Validation accuracy : 0.501\n",
            "batch no. 162, training accuracy 0.38\n",
            "batch no. 164, training accuracy 0.48\n",
            "batch no. 166, training accuracy 0.48\n",
            "batch no. 168, training accuracy 0.50\n",
            "batch no. 170, training accuracy 0.38\n",
            "Validation accuracy : 0.501\n",
            "batch no. 172, training accuracy 0.48\n",
            "batch no. 174, training accuracy 0.46\n",
            "batch no. 176, training accuracy 0.54\n",
            "batch no. 178, training accuracy 0.50\n",
            "batch no. 180, training accuracy 0.56\n",
            "Validation accuracy : 0.5\n",
            "batch no. 182, training accuracy 0.54\n",
            "batch no. 184, training accuracy 0.48\n",
            "batch no. 186, training accuracy 0.72\n",
            "batch no. 188, training accuracy 0.52\n",
            "batch no. 190, training accuracy 0.42\n",
            "Validation accuracy : 0.5\n",
            "batch no. 192, training accuracy 0.46\n",
            "batch no. 194, training accuracy 0.38\n",
            "batch no. 196, training accuracy 0.42\n",
            "batch no. 198, training accuracy 0.40\n",
            "batch no. 200, training accuracy 0.52\n",
            "Validation accuracy : 0.501\n",
            "batch no. 202, training accuracy 0.40\n",
            "batch no. 204, training accuracy 0.72\n",
            "batch no. 206, training accuracy 0.52\n",
            "batch no. 208, training accuracy 0.38\n",
            "batch no. 210, training accuracy 0.32\n",
            "Validation accuracy : 0.501\n",
            "batch no. 212, training accuracy 0.60\n",
            "batch no. 214, training accuracy 0.54\n",
            "batch no. 216, training accuracy 0.58\n",
            "batch no. 218, training accuracy 0.44\n",
            "batch no. 220, training accuracy 0.46\n",
            "Validation accuracy : 0.501\n",
            "batch no. 222, training accuracy 0.64\n",
            "batch no. 224, training accuracy 0.42\n",
            "batch no. 226, training accuracy 0.40\n",
            "batch no. 228, training accuracy 0.54\n",
            "batch no. 230, training accuracy 0.64\n",
            "Validation accuracy : 0.5\n",
            "batch no. 232, training accuracy 0.52\n",
            "batch no. 234, training accuracy 0.56\n",
            "batch no. 236, training accuracy 0.52\n",
            "batch no. 238, training accuracy 0.48\n",
            "batch no. 240, training accuracy 0.48\n",
            "Validation accuracy : 0.501\n",
            "batch no. 242, training accuracy 0.44\n",
            "batch no. 244, training accuracy 0.44\n",
            "batch no. 246, training accuracy 0.40\n",
            "batch no. 248, training accuracy 0.40\n",
            "batch no. 250, training accuracy 0.46\n",
            "Validation accuracy : 0.501\n",
            "batch no. 252, training accuracy 0.54\n",
            "batch no. 254, training accuracy 0.50\n",
            "batch no. 256, training accuracy 0.54\n",
            "batch no. 258, training accuracy 0.44\n",
            "batch no. 260, training accuracy 0.62\n",
            "Validation accuracy : 0.499\n",
            "batch no. 262, training accuracy 0.54\n",
            "batch no. 264, training accuracy 0.38\n",
            "batch no. 266, training accuracy 0.52\n",
            "batch no. 268, training accuracy 0.54\n",
            "batch no. 270, training accuracy 0.54\n",
            "Validation accuracy : 0.498\n",
            "batch no. 272, training accuracy 0.58\n",
            "batch no. 274, training accuracy 0.42\n",
            "batch no. 276, training accuracy 0.46\n",
            "batch no. 278, training accuracy 0.42\n",
            "batch no. 280, training accuracy 0.56\n",
            "Validation accuracy : 0.5\n",
            "batch no. 282, training accuracy 0.42\n",
            "batch no. 284, training accuracy 0.44\n",
            "batch no. 286, training accuracy 0.42\n",
            "batch no. 288, training accuracy 0.48\n",
            "batch no. 290, training accuracy 0.56\n",
            "Validation accuracy : 0.499\n",
            "batch no. 292, training accuracy 0.56\n",
            "batch no. 294, training accuracy 0.46\n",
            "batch no. 296, training accuracy 0.54\n",
            "batch no. 298, training accuracy 0.46\n",
            "batch no. 300, training accuracy 0.38\n",
            "Validation accuracy : 0.499\n",
            "batch no. 302, training accuracy 0.44\n",
            "batch no. 304, training accuracy 0.58\n",
            "batch no. 306, training accuracy 0.52\n",
            "batch no. 308, training accuracy 0.34\n",
            "batch no. 310, training accuracy 0.52\n",
            "Validation accuracy : 0.423\n",
            "batch no. 312, training accuracy 0.40\n",
            "batch no. 314, training accuracy 0.48\n",
            "batch no. 316, training accuracy 0.60\n",
            "batch no. 318, training accuracy 0.60\n",
            "batch no. 320, training accuracy 0.46\n",
            "Validation accuracy : 0.424\n",
            "batch no. 322, training accuracy 0.60\n",
            "batch no. 324, training accuracy 0.50\n",
            "batch no. 326, training accuracy 0.40\n",
            "batch no. 328, training accuracy 0.44\n",
            "batch no. 330, training accuracy 0.52\n",
            "Validation accuracy : 0.424\n",
            "batch no. 332, training accuracy 0.42\n",
            "batch no. 334, training accuracy 0.50\n",
            "batch no. 336, training accuracy 0.48\n",
            "batch no. 338, training accuracy 0.44\n",
            "batch no. 340, training accuracy 0.54\n",
            "Validation accuracy : 0.424\n",
            "batch no. 342, training accuracy 0.48\n",
            "batch no. 344, training accuracy 0.64\n",
            "batch no. 346, training accuracy 0.48\n",
            "batch no. 348, training accuracy 0.56\n",
            "batch no. 350, training accuracy 0.60\n",
            "Validation accuracy : 0.424\n",
            "batch no. 352, training accuracy 0.48\n",
            "batch no. 354, training accuracy 0.46\n",
            "batch no. 356, training accuracy 0.44\n",
            "batch no. 358, training accuracy 0.52\n",
            "batch no. 360, training accuracy 0.54\n",
            "Validation accuracy : 0.424\n",
            "batch no. 362, training accuracy 0.52\n",
            "batch no. 364, training accuracy 0.50\n",
            "batch no. 366, training accuracy 0.44\n",
            "batch no. 368, training accuracy 0.46\n",
            "batch no. 370, training accuracy 0.46\n",
            "Validation accuracy : 0.424\n",
            "batch no. 372, training accuracy 0.60\n",
            "batch no. 374, training accuracy 0.52\n",
            "batch no. 376, training accuracy 0.46\n",
            "batch no. 378, training accuracy 0.44\n",
            "batch no. 380, training accuracy 0.40\n",
            "Validation accuracy : 0.424\n",
            "batch no. 382, training accuracy 0.58\n",
            "batch no. 384, training accuracy 0.62\n",
            "batch no. 386, training accuracy 0.64\n",
            "batch no. 388, training accuracy 0.48\n",
            "batch no. 390, training accuracy 0.48\n",
            "Validation accuracy : 0.424\n",
            "batch no. 392, training accuracy 0.42\n",
            "batch no. 394, training accuracy 0.44\n",
            "batch no. 396, training accuracy 0.40\n",
            "batch no. 398, training accuracy 0.54\n",
            "batch no. 400, training accuracy 0.48\n",
            "Validation accuracy : 0.424\n",
            "Epoch 2\n",
            "batch no. 2, training accuracy 0.40\n",
            "batch no. 4, training accuracy 0.46\n",
            "batch no. 6, training accuracy 0.48\n",
            "batch no. 8, training accuracy 0.56\n",
            "batch no. 10, training accuracy 0.44\n",
            "Validation accuracy : 0.424\n",
            "batch no. 12, training accuracy 0.52\n",
            "batch no. 14, training accuracy 0.34\n",
            "batch no. 16, training accuracy 0.42\n",
            "batch no. 18, training accuracy 0.54\n",
            "batch no. 20, training accuracy 0.54\n",
            "Validation accuracy : 0.424\n",
            "batch no. 22, training accuracy 0.54\n",
            "batch no. 24, training accuracy 0.54\n",
            "batch no. 26, training accuracy 0.54\n",
            "batch no. 28, training accuracy 0.56\n",
            "batch no. 30, training accuracy 0.56\n",
            "Validation accuracy : 0.424\n",
            "batch no. 32, training accuracy 0.58\n",
            "batch no. 34, training accuracy 0.46\n",
            "batch no. 36, training accuracy 0.44\n",
            "batch no. 38, training accuracy 0.48\n",
            "batch no. 40, training accuracy 0.58\n",
            "Validation accuracy : 0.424\n",
            "batch no. 42, training accuracy 0.38\n",
            "batch no. 44, training accuracy 0.52\n",
            "batch no. 46, training accuracy 0.58\n",
            "batch no. 48, training accuracy 0.54\n",
            "batch no. 50, training accuracy 0.48\n",
            "Validation accuracy : 0.424\n",
            "batch no. 52, training accuracy 0.44\n",
            "batch no. 54, training accuracy 0.44\n",
            "batch no. 56, training accuracy 0.54\n",
            "batch no. 58, training accuracy 0.46\n",
            "batch no. 60, training accuracy 0.52\n",
            "Validation accuracy : 0.424\n",
            "batch no. 62, training accuracy 0.52\n",
            "batch no. 64, training accuracy 0.52\n",
            "batch no. 66, training accuracy 0.48\n",
            "batch no. 68, training accuracy 0.52\n",
            "batch no. 70, training accuracy 0.50\n",
            "Validation accuracy : 0.424\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-3e2f29c6ddc9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_lengths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0;31m# print(\"Input review texts, text_lengths, labels\", texts.shape, text_lengths.shape, labels.shape)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mprediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtexts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtext_lengths\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0;31m# print(\"Prediction\", prediction.item())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m             \u001b[0;31m# raise ValueError('Not done')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-15-f076fbcd961c>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, pack)\u001b[0m\n\u001b[1;32m    173\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0m_r\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwords_len\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m                     \u001b[0membedded_word\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membedded\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 175\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlstm_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedded_word\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mh\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    176\u001b[0m                     \u001b[0mword_index\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m                 \u001b[0;31m# print('word_index', word_index, 'tokens_read', tokens_read, 'jumps_made', jumps_made)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, hx)\u001b[0m\n\u001b[1;32m    888\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_ih\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight_hh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_ih\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias_hh\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    891\u001b[0m         )\n\u001b[1;32m    892\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0zkubi0BCcnr",
        "colab": {}
      },
      "source": [
        "torch.save(policy_model.state_dict(), 'fast_text_model.weights')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kR9t1X7Q3hdm",
        "colab_type": "text"
      },
      "source": [
        "# Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "reYP11IuGriR",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 320
        },
        "outputId": "b37d3fdf-fe74-413c-8d58-60c6f1bc53e4"
      },
      "source": [
        "import timeit\n",
        "import spacy\n",
        "import matplotlib.pyplot as plt\n",
        "nlp = spacy.load('en')\n",
        "\n",
        "def predict_sentiment(model, sentence):\n",
        "    model.train_mode = False\n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
        "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
        "    # tensor = torch.LongTensor(indexed).to(device)\n",
        "    tensor = torch.LongTensor(indexed).to('cpu')\n",
        "    tensor = tensor.unsqueeze(1)\n",
        "    model((tensor, torch.tensor([tensor.shape[0]]), None))\n",
        "    res = torch.sigmoid(model.label_predictions[0])\n",
        "    model.train_mode = False\n",
        "    return res\n",
        "\n",
        "times = []\n",
        "lengths=[]\n",
        "train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size=1,\n",
        "    device=device,\n",
        "    shuffle = True,\n",
        "    # sort_key=lambda x: len(x.text),\n",
        "    sort_within_batch=False)\n",
        "\n",
        "i = 0\n",
        "for _data in test_iterator:\n",
        "    if i%100 == 0:\n",
        "        # get the inputs\n",
        "        texts, text_lengths, labels = _data.text[0], _data.text[1], _data.label\n",
        "        #print(texts.shape, text_lengths.shape, labels.shape)\n",
        "        start_time = timeit.default_timer()\n",
        "        predictions = policy_model((texts.to(device), text_lengths.to(device), labels.to(device)))\n",
        "        elapsed = timeit.default_timer() - start_time\n",
        "        lengths.append(texts.shape[0])\n",
        "        times.append(elapsed)\n",
        "        # print(\"Input review texts, text_lengths, labels\", texts.shape, text_lengths.shape, labels.shape)\n",
        "    if i>20000:\n",
        "        break\n",
        "    i += 1\n",
        "\n",
        "import pickle\n",
        "\n",
        "pickle_out = open(\"test_times_1.pickle\",\"wb\")\n",
        "pickle.dump((lengths, times), pickle_out)\n",
        "pickle_out.close()\n",
        "\n",
        "plt.scatter(lengths, times, label='skip-model')\n",
        "plt.xlabel('Lengths of sentences')\n",
        "plt.ylabel('Time taken for prediction')\n",
        "plt.show()"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:210: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZQAAAEKCAYAAAA1qaOTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "1b4DUH5SGyNU",
        "colab": {}
      },
      "source": [
        "predict_sentiment(policy_model, \"This film is terrible what can I say\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xn9ky27x-C_e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "\n",
        "pickle_out = open(\"training_epochs_1.pickle\",\"wb\")\n",
        "pickle.dump((policy_model.training_accuracies, policy_model.validation_accuracies), pickle_out)\n",
        "pickle_out.close()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IaA0E6da-cAO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}